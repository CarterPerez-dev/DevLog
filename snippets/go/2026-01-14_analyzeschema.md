# AnalyzeSchema

**Repository:** oneIsNun_
**File:** go-backend/internal/mongodb/collections.go
**Language:** go
**Lines:** 208-281
**Complexity:** 10.0

---

## Source Code

```go
func (r *CollectionsRepository) AnalyzeSchema(ctx context.Context, dbName, collName string, sampleSize int) (*SchemaAnalysis, error) {
	db := r.client.client.Database(dbName)
	coll := db.Collection(collName)

	totalDocs, err := coll.EstimatedDocumentCount(ctx)
	if err != nil {
		return nil, fmt.Errorf("count documents: %w", err)
	}

	if sampleSize <= 0 {
		sampleSize = 1000
	}
	if int64(sampleSize) > totalDocs {
		sampleSize = int(totalDocs)
	}

	pipeline := mongo.Pipeline{
		{{Key: "$sample", Value: bson.D{{Key: "size", Value: sampleSize}}}},
	}

	cursor, err := coll.Aggregate(ctx, pipeline)
	if err != nil {
		return nil, fmt.Errorf("sample documents: %w", err)
	}
	defer cursor.Close(ctx)

	fieldMap := make(map[string]*fieldInfo)
	var sampledCount int64

	for cursor.Next(ctx) {
		var doc bson.M
		if err := cursor.Decode(&doc); err != nil {
			continue
		}
		sampledCount++
		analyzeDocument("", doc, fieldMap)
	}

	var fields []FieldSchema
	for name, info := range fieldMap {
		typeList := make([]string, 0, len(info.types))
		for t := range info.types {
			typeList = append(typeList, t)
		}
		sort.Strings(typeList)

		coverage := float64(info.count) / float64(sampledCount) * 100

		samples := info.samples
		if len(samples) > 5 {
			samples = samples[:5]
		}

		fields = append(fields, FieldSchema{
			Name:         name,
			Types:        typeList,
			Coverage:     coverage,
			Count:        info.count,
			TotalDocs:    sampledCount,
			SampleValues: samples,
		})
	}

	sort.Slice(fields, func(i, j int) bool {
		return fields[i].Coverage > fields[j].Coverage
	})

	return &SchemaAnalysis{
		CollectionName: collName,
		TotalDocuments: totalDocs,
		SampleSize:     sampledCount,
		Fields:         fields,
	}, nil
}
```

---

## Documentation

### Documentation for `AnalyzeSchema` Function

**Purpose and Behavior:**
The `AnalyzeSchema` function in the `CollectionsRepository` class analyzes the schema of a MongoDB collection by sampling documents and determining field types, coverage, and sample values. It returns a structured analysis report.

**Key Implementation Details:**
1. **Error Handling:** The function uses context-aware error handling to manage potential issues during database operations.
2. **Sampling Logic:** Documents are sampled based on the specified `sampleSize`, with a default of 1000 if not provided or invalid.
3. **Pipeline Construction:** A MongoDB aggregation pipeline is constructed to sample documents, which are then decoded and analyzed.
4. **Field Analysis:** Each field's type, coverage, and sample values are calculated from the sampled documents.

**When/Why to Use:**
Use this function when you need a detailed schema analysis of a collection without loading all documents into memory. It is particularly useful for large datasets where full scans would be inefficient or impractical.

**Patterns/Gotchas:**
- **Sampling Limitations:** The sample size can significantly affect the accuracy and representativeness of the field types and coverage.
- **Resource Management:** Ensure that the context passed to `cursor.Close(ctx)` is properly managed to avoid resource leaks.
- **Default Sample Size:** A default sample size of 1000 documents may not be optimal for all use cases; consider adjusting this value based on collection size and requirements.

---

*Generated by CodeWorm on 2026-01-14 01:14*
