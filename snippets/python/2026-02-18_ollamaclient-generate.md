# OllamaClient.generate

**Type:** Documentation
**Repository:** CodeWorm
**File:** codeworm/llm/client.py
**Language:** python
**Lines:** 147-212
**Complexity:** 11.0

---

## Source Code

```python
async def generate(
        self,
        prompt: str,
        system: str | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> GenerationResult:
        """
        Generate text from a prompt
        """
        client = await self._get_client()

        options = {
            "temperature": temperature or self.settings.temperature,
            "num_predict": max_tokens or self.settings.num_predict,
            "num_ctx": self.settings.num_ctx,
        }

        payload = {
            "model": self.settings.model,
            "prompt": prompt,
            "stream": False,
            "keep_alive": self.settings.keep_alive,
            "options": options,
        }

        if system:
            payload["system"] = system

        try:
            response = await client.post("/api/generate", json = payload)

            if response.status_code != 200:
                error_text = response.text
                if "out of memory" in error_text.lower(
                ) or "cuda" in error_text.lower():
                    raise OllamaModelError(f"Model OOM: {error_text}")
                raise OllamaError(f"Generation failed: {error_text}")

            data = response.json()

            prompt_tokens = data.get("prompt_eval_count", 0)
            completion_tokens = data.get("eval_count", 0)
            total_duration = data.get("total_duration", 0) / 1_000_000

            tokens_per_sec = 0.0
            if total_duration > 0 and completion_tokens > 0:
                tokens_per_sec = completion_tokens / (total_duration / 1000)

            return GenerationResult(
                text = data.get("response",
                                ""),
                model = data.get("model",
                                 self.settings.model),
                prompt_tokens = prompt_tokens,
                completion_tokens = completion_tokens,
                total_duration_ms = int(total_duration),
                tokens_per_second = tokens_per_sec,
            )

        except httpx.ConnectError as e:
            raise OllamaConnectionError(
                f"Cannot connect to Ollama at {self.base_url}: {e}"
            ) from e
        except httpx.TimeoutException as e:
            raise OllamaTimeoutError(f"Request timed out: {e}") from e
```

---

## Documentation

### Documentation for `generate` Method in OllamaClient

**Purpose and Behavior:**
The `generate` method is an asynchronous function that sends a request to the Ollama API to generate text based on a given prompt. It constructs payload options, including temperature, max tokens, and system context if provided, and handles the response by parsing JSON data into a `GenerationResult` object.

**Key Implementation Details:**
- **Asynchronous Handling:** Uses `async/await` for making HTTP requests.
- **Default Values:** Utilizes default values for optional parameters like `temperature`, `max_tokens`, and `system`.
- **Error Handling:** Catches specific exceptions (`httpx.ConnectError`, `httpx.TimeoutException`) to handle connection issues and timeouts gracefully, raising custom error classes.

**When/Why to Use:**
This method is used when you need to generate text from a prompt using the Ollama API. It's particularly useful in scenarios where you require detailed control over the generation process, such as setting temperature or specifying max tokens.

**Patterns and Gotchas:**
- **Custom Error Handling:** The use of custom error classes (`OllamaModelError`, `OllamaError`, `OllamaConnectionError`, `OllamaTimeoutError`) ensures clear and specific error messages.
- **Resource Management:** Properly handles HTTP requests asynchronously, ensuring efficient resource usage.

---

*Generated by CodeWorm on 2026-02-18 09:06*
