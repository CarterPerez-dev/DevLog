# train_autoencoder

**Type:** Documentation
**Repository:** Cybersecurity-Projects
**File:** PROJECTS/advanced/ai-threat-detection/backend/ml/train_autoencoder.py
**Language:** python
**Lines:** 16-111
**Complexity:** 7.0

---

## Source Code

```python
def train_autoencoder(
    X_normal: np.ndarray,
    epochs: int = 100,
    batch_size: int = 256,
    lr: float = 1e-3,
    percentile: float = 99.5,
    val_split: float = 0.15,
    patience: int = 10,
) -> dict[str, Any]:
    """
    Train the autoencoder on normal-only traffic and calibrate the
    anomaly detection threshold.
    """
    input_dim = X_normal.shape[1]

    split_idx = int(len(X_normal) * (1 - val_split))
    X_train_raw = X_normal[:split_idx]
    X_val_raw = X_normal[split_idx:]

    scaler = FeatureScaler()
    X_train_scaled = scaler.fit_transform(X_train_raw)
    X_val_scaled = scaler.transform(X_val_raw)

    train_tensor = torch.from_numpy(X_train_scaled)
    val_tensor = torch.from_numpy(X_val_scaled)

    train_loader = DataLoader(
        TensorDataset(train_tensor),
        batch_size=batch_size,
        shuffle=True,
        drop_last=len(train_tensor) > batch_size,
    )

    model = ThreatAutoencoder(input_dim=input_dim)
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=lr, weight_decay=1e-5, betas=(0.9, 0.999)
    )
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=5, min_lr=1e-6
    )

    history: dict[str, list[float]] = {"train_loss": [], "val_loss": []}
    best_val_loss = float("inf")
    best_state = None
    epochs_without_improvement = 0

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        n_batches = 0

        for (batch,) in train_loader:
            reconstructed = model(batch)
            loss = torch.nn.functional.mse_loss(reconstructed, batch)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            epoch_loss += loss.item()
            n_batches += 1

        avg_train_loss = epoch_loss / max(n_batches, 1)
        history["train_loss"].append(avg_train_loss)

        model.eval()
        with torch.no_grad():
            val_reconstructed = model(val_tensor)
            val_loss = torch.nn.functional.mse_loss(val_reconstructed, val_tensor).item()
        history["val_loss"].append(val_loss)

        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_state = {k: v.clone() for k, v in model.state_dict().items()}
            epochs_without_improvement = 0
        else:
            epochs_without_improvement += 1

        if epochs_without_improvement >= patience:
            break

    if best_state is not None:
        model.load_state_dict(best_state)

    model.eval()
    with torch.no_grad():
        val_errors = model.compute_reconstruction_error(val_tensor)
    threshold = float(np.percentile(val_errors.numpy(), percentile))

    return {
        "model": model,
        "scaler": scaler,
        "threshold": threshold,
        "history": history,
    }
```

---

## Documentation

### Documentation for `train_autoencoder`

**Purpose and Behavior:**
The `train_autoencoder` function trains an autoencoder on normal traffic data, evaluates its performance, and calibrates the anomaly detection threshold using a validation set. It returns a dictionary containing the trained model, scaler, threshold, and training history.

**Key Implementation Details:**
- **Data Preparation:** Splits input data into training and validation sets, scales features, and prepares PyTorch tensors and dataloaders.
- **Model Training:** Uses AdamW optimizer with gradient clipping and ReduceLROnPlateau scheduler. Trains the model for a specified number of epochs, tracking loss on both train and validation datasets.
- **Threshold Calculation:** Computes reconstruction errors on the validation set and sets an anomaly detection threshold based on the 99.5th percentile.

**When/Why to Use:**
Use this function when you need to train an autoencoder for detecting anomalies in network traffic data, particularly in cybersecurity applications where normal behavior patterns are known. It is suitable for scenarios requiring real-time threat detection and calibration of anomaly thresholds.

**Patterns/Gotchas:**
- **Resource Management:** The `model.eval()` and `torch.no_grad()` context managers ensure that the model operates in evaluation mode without unnecessary computations.
- **Early Stopping:** Implements early stopping based on validation loss to prevent overfitting, which is crucial for maintaining model generalization.

---

*Generated by CodeWorm on 2026-02-26 00:57*
