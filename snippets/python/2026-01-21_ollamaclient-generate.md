# OllamaClient.generate

**Repository:** CertGames-Core
**File:** backend/api/domains/tools/ollama/services/ollama_client.py
**Language:** python
**Lines:** 59-104
**Complexity:** 7.0

---

## Source Code

```python
def generate(
        prompt: str,
        model: str | None = None,
        *,
        stream: bool = True,
        format: str | dict[str,
                           Any] | None = None,
        system: str | None = None,
        temperature: float = 0.7,
        num_predict: int | None = None,
        **options
    ) -> dict[str, Any] | Generator[str]:
        """
        Call /api/generate endpoint
        """
        if model is None:
            model = DEFAULT_MODEL

        opts: dict[str, Any] = {"temperature": temperature, **options}
        if num_predict is not None:
            opts["num_predict"] = num_predict

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": opts
        }

        if format:
            payload["format"] = format
        if system:
            payload["system"] = system

        url = f"{OLLAMA_BASE_URL}/api/generate"

        try:
            if stream:
                return OllamaClient._stream_response(url, payload)

            response = requests.post(url, json = payload, timeout = 120)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error("Ollama API error: %s", str(e))
            raise
```

---

## Documentation

### Documentation for `OllamaClient.generate`

**Purpose and Behavior:**
The `generate` method sends a POST request to the `/api/generate` endpoint of an Ollama model server, using provided parameters such as prompt, model, stream options, format, system message, temperature, and other custom options. It handles both streaming responses and non-streaming (full) responses.

**Key Implementation Details:**
- The method accepts a `prompt` string and various optional parameters including `model`, `stream`, `format`, `system`, `temperature`, and additional `options`.
- It constructs the request payload dynamically based on the provided arguments.
- If `stream` is `True`, it uses `_stream_response` to handle streaming responses; otherwise, it sends a full POST request and returns the JSON response.

**When/Why to Use:**
Use this method when you need to interact with an Ollama model server for generating text based on given prompts. It's particularly useful in scenarios where real-time feedback is required or when dealing with large models that might benefit from streaming responses.

**Patterns and Gotchas:**
- The use of type hints (`str`, `float`, `dict[str, Any]`) ensures clear parameter expectations.
- Dynamic payload construction via unpacking dictionaries helps manage optional parameters efficiently.
- Error handling includes logging and re-raising exceptions for robust error management. Note that the `_stream_response` method must be defined elsewhere in the codebase to function correctly.

This method is well-suited for integrating text generation capabilities into applications, providing flexibility with its parameter options.

---

*Generated by CodeWorm on 2026-01-21 18:12*
