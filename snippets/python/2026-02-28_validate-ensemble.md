# validate_ensemble

**Type:** Documentation
**Repository:** Cybersecurity-Projects
**File:** PROJECTS/advanced/ai-threat-detection/backend/ml/validation.py
**Language:** python
**Lines:** 54-113
**Complexity:** 5.0

---

## Source Code

```python
def validate_ensemble(
    model_dir: Path,
    X_test: np.ndarray,
    y_test: np.ndarray,
    ensemble_weights: dict[str, float] | None = None,
    pr_auc_gate: float = 0.85,
    f1_gate: float = 0.80,
) -> ValidationResult:
    """
    Run all 3 models on test data and compute classification metrics
    """
    weights = ensemble_weights or DEFAULT_ENSEMBLE_WEIGHTS

    engine = InferenceEngine(model_dir=str(model_dir))
    if not engine.is_loaded:
        raise RuntimeError(f"Failed to load models from {model_dir}")

    raw_scores = engine.predict(X_test.astype(np.float32), )
    if raw_scores is None:
        raise RuntimeError("Inference returned None")

    fused = _compute_fused_scores(raw_scores, engine.threshold, weights)

    y_pred = (fused >= BINARY_THRESHOLD).astype(np.int32)

    prec = float(precision_score(y_test, y_pred, zero_division=0))
    rec = float(recall_score(y_test, y_pred, zero_division=0))
    f1_val = float(f1_score(y_test, y_pred, zero_division=0))
    pr_auc_val = float(average_precision_score(y_test, fused))
    roc_auc_val = float(roc_auc_score(y_test, fused))

    cm = confusion_matrix(y_test, y_pred).tolist()

    pr_auc_passed = pr_auc_val >= pr_auc_gate
    f1_passed = f1_val >= f1_gate
    gate_details = {
        "pr_auc": pr_auc_passed,
        "f1": f1_passed,
    }

    logger.info(
        "Validation: precision=%.3f recall=%.3f "
        "f1=%.3f pr_auc=%.3f roc_auc=%.3f",
        prec,
        rec,
        f1_val,
        pr_auc_val,
        roc_auc_val,
    )

    return ValidationResult(
        precision=prec,
        recall=rec,
        f1=f1_val,
        pr_auc=pr_auc_val,
        roc_auc=roc_auc_val,
        confusion_matrix=cm,
        passed_gates=pr_auc_passed and f1_passed,
        gate_details=gate_details,
    )
```

---

## Documentation

### Documentation for `validate_ensemble`

**Purpose and Behavior:**
The `validate_ensemble` function evaluates the performance of a machine learning model ensemble on test data by computing various classification metrics, including precision, recall, F1-score, PR AUC, ROC AUC, and confusion matrix. It also checks if these metrics meet predefined thresholds.

**Key Implementation Details:**
- **Inputs:** Accepts model directory (`model_dir`), test features (`X_test`), test labels (`y_test`), optional ensemble weights (`ensemble_weights`), and threshold values for PR AUC and F1-score gates.
- **Process:** Loads models, predicts raw scores, fuses them using provided or default weights, and calculates final predictions. Metrics are computed based on these predictions and test labels.
- **Output:** Returns a `ValidationResult` object containing all calculated metrics and gate pass status.

**When/Why to Use:**
Use this function during model validation in cybersecurity projects where ensemble performance needs rigorous evaluation against specific thresholds for PR AUC and F1-score. It ensures that the models meet critical performance criteria before deployment.

**Patterns/Gotchas:**
- Ensure `ensemble_weights` are correctly defined; otherwise, default values will be used.
- The function relies on proper model loading and prediction behavior from `InferenceEngine`.
- Metrics like PR AUC and ROC AUC can be sensitive to threshold settings. Adjust `BINARY_THRESHOLD` as needed for your specific use case.

---

*Generated by CodeWorm on 2026-02-28 09:31*
