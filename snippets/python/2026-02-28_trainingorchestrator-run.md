# TrainingOrchestrator.run

**Type:** Documentation
**Repository:** Cybersecurity-Projects
**File:** PROJECTS/advanced/ai-threat-detection/backend/ml/orchestrator.py
**Language:** python
**Lines:** 73-173
**Complexity:** 4.0

---

## Source Code

```python
def run(
        self,
        X: np.ndarray,
        y: np.ndarray,
    ) -> TrainingResult:
        """
        Execute the full training pipeline
        """
        self._output_dir.mkdir(parents=True, exist_ok=True)

        split = prepare_training_data(X, y)

        logger.info(
            "Split: train=%d val=%d test=%d normal_train=%d",
            len(split.X_train),
            len(split.X_val),
            len(split.X_test),
            len(split.X_normal_train),
        )

        with VigilExperiment(self._experiment_name) as experiment:
            experiment.log_params({
                "epochs": self._epochs,
                "batch_size": self._batch_size,
                "n_samples": len(X),
                "n_attack": int(np.sum(y == 1)),
                "n_normal": int(np.sum(y == 0)),
                "n_features": X.shape[1],
            })

            ae_result = self._train_ae(split.X_normal_train)
            ae_metrics = {
                "ae_threshold": ae_result["threshold"],
                "ae_final_train_loss": ae_result["history"]["train_loss"][-1],
                "ae_final_val_loss": ae_result["history"]["val_loss"][-1],
            }

            rf_result = self._train_rf(split.X_train, split.y_train)
            rf_metrics = rf_result["metrics"]

            if_result = self._train_if(split.X_normal_train)
            if_metrics = if_result["metrics"]

            self._export_models(ae_result, rf_result, if_result)

            experiment.log_metrics(ae_metrics)
            experiment.log_metrics({
                f"rf_{k}": v
                for k, v in rf_metrics.items()
            })

            try:
                ensemble = validate_ensemble(
                    self._output_dir,
                    split.X_test,
                    split.y_test,
                )
                experiment.log_metrics({
                    "ensemble_precision": ensemble.precision,
                    "ensemble_recall": ensemble.recall,
                    "ensemble_f1": ensemble.f1,
                    "ensemble_pr_auc": ensemble.pr_auc,
                    "ensemble_roc_auc": ensemble.roc_auc,
                })
                passed = ensemble.passed_gates
            except Exception as exc:
                logger.exception("Ensemble validation failed")
                print(
                    f"  WARNING: validation raised"
                    f" {type(exc).__name__}: {exc}",
                    file=sys.stderr,
                )
                ensemble = None
                passed = False

            for name in (
                AE_FILENAME,
                RF_FILENAME,
                IF_FILENAME,
                SCALER_FILENAME,
                THRESHOLD_FILENAME,
            ):
                experiment.log_artifact(self._output_dir / name)

            run_id = experiment.run_id

        logger.info(
            "Training complete: passed_gates=%s run_id=%s",
            passed,
            run_id,
```

---

## Documentation

### Documentation for `TrainingOrchestrator.run`

**Purpose and Behavior:**
The `run` method executes the full training pipeline, including data splitting, model training (Autoencoder, Random Forest, Isolation Forest), ensemble validation, and logging metrics to an experiment tracker. It returns a structured `TrainingResult` object containing various performance metrics.

**Key Implementation Details:**
- **Data Splitting:** Uses `prepare_training_data` to split the input dataset into train, validation, test, and normal training sets.
- **Experiment Tracking:** Logs parameters and metrics using a `VigilExperiment` context manager. Artifacts are also logged post-training.
- **Exception Handling:** Catches exceptions during ensemble validation and logs them for debugging.

**When/Why to Use:**
This method is ideal for orchestrating the end-to-end training process in cybersecurity threat detection projects, ensuring comprehensive logging and artifact management. It's particularly useful when integrating multiple machine learning models into a cohesive pipeline.

**Patterns/Gotchas:**
- **Context Managers:** The `VigilExperiment` context manager ensures proper logging and cleanup.
- **Exception Handling:** Robust error handling is crucial for real-world applications, as it allows the process to continue even if validation fails.
- **Performance Metrics Logging:** Comprehensive metrics are logged, aiding in post-training analysis and model comparison.

---

*Generated by CodeWorm on 2026-02-28 11:39*
