# WindowAggregator.record_and_aggregate

**Type:** Documentation
**Repository:** Cybersecurity-Projects
**File:** PROJECTS/advanced/ai-threat-detection/backend/app/core/features/aggregator.py
**Language:** python
**Lines:** 33-121
**Complexity:** 3.0

---

## Source Code

```python
async def record_and_aggregate(
        self,
        ip: str,
        request_id: str,
        path: str,
        path_depth: int,
        method: str,
        status_code: int,
        user_agent: str,
        response_size: int,
        timestamp: float,
    ) -> dict[str, float]:
        """
        Record a request into Redis sorted sets and return all 12
        per-IP windowed features.
        """
        prefix = f"ip:{ip}"
        keys = {
            "requests": f"{prefix}:requests",
            "paths": f"{prefix}:paths",
            "statuses": f"{prefix}:statuses",
            "uas": f"{prefix}:uas",
            "sizes": f"{prefix}:sizes",
            "methods": f"{prefix}:methods",
            "depths": f"{prefix}:depths",
        }

        trim_boundary = timestamp - KEY_TTL
        w1m = timestamp - WINDOW_1M
        w5m = timestamp - WINDOW_5M
        w10m = timestamp - WINDOW_10M

        pipe = self._redis.pipeline()

        pipe.zadd(keys["requests"], {request_id: timestamp})
        pipe.zadd(keys["paths"], {_hash_member(path): timestamp})
        pipe.zadd(keys["statuses"], {f"{status_code}:{request_id}": timestamp})
        pipe.zadd(keys["uas"], {_hash_member(user_agent): timestamp})
        pipe.zadd(keys["sizes"], {f"{response_size}:{request_id}": timestamp})
        pipe.zadd(keys["methods"], {f"{method}:{request_id}": timestamp})
        pipe.zadd(keys["depths"], {f"{path_depth}:{request_id}": timestamp})

        for key in keys.values():
            pipe.zremrangebyscore(key, "-inf", trim_boundary)

        pipe.zcount(keys["requests"], w1m, "+inf")
        pipe.zcount(keys["requests"], w5m, "+inf")
        pipe.zcount(keys["requests"], w10m, "+inf")
        pipe.zcount(keys["paths"], w5m, "+inf")
        pipe.zcount(keys["uas"], w10m, "+inf")
        pipe.zrangebyscore(keys["statuses"], w5m, "+inf")
        pipe.zrangebyscore(keys["sizes"], w5m, "+inf")
        pipe.zrangebyscore(keys["methods"], w5m, "+inf")
        pipe.zrangebyscore(keys["depths"], w5m, "+inf")
        pipe.zrangebyscore(keys["requests"], w10m, "+inf", withscores=True)

        for key in keys.values():
            pipe.expire(key, KEY_TTL)

        results = await pipe.execute()

        read_start = 14
        req_count_1m = results[read_start]
        req_count_5m = results[read_start + 1]
        req_count_10m = results[read_start + 2]
        unique_paths_5m = results[read_start + 3]
        unique_uas_10m = results[read_start + 4]
        statuses_5m = results[read_start + 5]
        sizes_5m = results[read_start + 6]
        methods_5m = results[read_start + 7]
        depths_5m = results[read_start + 8]
        requests_with_scores = results[read_start + 9]

        irt_mean, irt_std = _inter_request_time_stats(requests_with_scores)

        return {
            "req_count_1m": float(req_count_1m),
            "req_count_5m": float(req_count_5m),
            "req_count_10m": float(req_count_10m),
            "error_rate_5m": _error_rat
```

---

## Documentation

### Documentation for `record_and_aggregate`

**Purpose and Behavior:**
The function `record_and_aggregate` records a request into Redis sorted sets, then calculates 12 per-IP windowed features over different time windows (1m, 5m, 10m). It returns these metrics as a dictionary.

**Key Implementation Details:**
- **Redis Operations:** Uses Redis pipelines for efficient batch operations.
- **Time Windows:** Implements sliding windows using timestamps to count requests and unique paths over the last 1 minute, 5 minutes, and 10 minutes.
- **Metrics Calculation:** Computes various metrics such as request counts, error rates, path depth variance, inter-request time statistics, etc.

**When/Why to Use:**
Use this function in scenarios requiring real-time monitoring of web server performance or threat detection. It's particularly useful for aggregating and analyzing HTTP requests based on IP addresses over different time windows.

**Patterns/Gotchas:**
- **Timestamp Management:** Ensure `timestamp` is accurate as it drives the sliding window logic.
- **Redis Key Expiration:** Keys are set to expire after a certain TTL, which needs to be managed carefully to avoid data loss.
- **Error Handling:** While not shown here, consider adding error handling for Redis operations and metric calculations.

This function encapsulates complex time-series analysis in a clean, modular manner.

---

*Generated by CodeWorm on 2026-02-28 03:00*
