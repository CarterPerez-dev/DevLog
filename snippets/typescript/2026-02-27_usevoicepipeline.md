# useVoicePipeline

**Type:** Documentation
**Repository:** angelamos-3d
**File:** frontend/src/hooks/useVoicePipeline.ts
**Language:** typescript
**Lines:** 37-285
**Complexity:** 1.0

---

## Source Code

```typescript
function useVoicePipeline({
	animControllerRef,
	animManagerRef,
	analyserRef,
	dataArrayRef,
	isPlayingRef,
	wakeWordRef,
	recorderRef,
	onStatusChange,
	onTranscriptChange,
	onResponseChange,
	onError,
}: UseVoicePipelineProps) {
	const audioContextRef = useRef<AudioContext | null>(null);
	const sourceRef = useRef<AudioBufferSourceNode | null>(null);
	const messagesRef = useRef<OllamaMessage[]>([]);
	const statusRef = useRef<AngelaStatus>("initializing");
	const abortControllerRef = useRef<AbortController | null>(null);

	const updateStatus = useCallback(
		(newStatus: AngelaStatus) => {
			statusRef.current = newStatus;
			onStatusChange(newStatus);

			const state = newStatus === "processing" ? "thinking" : newStatus;

			if (animControllerRef.current) {
				animControllerRef.current.setState(
					state as "idle" | "listening" | "thinking" | "speaking" | "error",
				);
			}

			if (animManagerRef.current) {
				animManagerRef.current.setState(
					state as "idle" | "listening" | "thinking" | "speaking" | "error",
				);
			}
		},
		[animControllerRef, animManagerRef, onStatusChange],
	);

	const playAudioWithLipSync = useCallback(
		async (audioBuffer: ArrayBuffer): Promise<void> => {
			audioContextRef.current = new AudioContext();
			analyserRef.current = audioContextRef.current.createAnalyser();
			analyserRef.current.fftSize = 256;
			dataArrayRef.current = new Uint8Array(
				analyserRef.current.frequencyBinCount,
			);

			const decoded = await audioContextRef.current.decodeAudioData(
				audioBuffer.slice(0),
			);

			sourceRef.current = audioContextRef.current.createBufferSource();
			sourceRef.current.buffer = decoded;
			sourceRef.current.connect(analyserRef.current);
			analyserRef.current.connect(audioContextRef.current.destination);

			isPlayingRef.current = true;

			return new Promise((resolve) => {
				const source = sourceRef.current;
				if (!source) {
					resolve();
					return;
				}
				source.onended = () => {
					isPlayingRef.current = false;
					animControllerRef.current?.setMouthOpen(0);
					audioContextRef.current?.close();
					resolve();
				};
				source.start(0);
			});
		},
		[analyserRef, dataArrayRef, isPlayingRef, animControllerRef],
	);

	const processAudio = useCallback(
		async (audioBlob: Blob) => {
			updateStatus("processing");

			try {
				logger.pipeline.log("Transcribing...");
				const result = await transcribeAudio(audioBlob);
				logger.pipeline.log("Transcription:", result.text);

				if (!result.text?.trim()) {
					logger.pipeline.log("Empty transcription");
					updateStatus("idle");
					wakeWordRef.current?.start();
					return;
				}

				onTranscriptChange(result.text);
				messagesRef.current.push({ role: "user", content: result.text });

				updateStatus("thinking");
				logger.pipeline.log("Calling LLM...");

				abortControllerRef.current = new AbortController();

				let fullResponse = "";
				try {
					for await (const chunk of streamChat(
						messagesRef.current,
						
```

---

## Documentation

### Documentation for `useVoicePipeline`

**Purpose and Behavior:**
The `useVoicePipeline` function handles the entire voice interaction pipeline, including audio processing, transcription, LLM response handling, and speech synthesis. It updates the application state based on various status changes and interacts with external components like animation controllers.

**Key Implementation Details:**
- **Status Management:** The function uses a `statusRef` to manage the current state of the pipeline.
- **Audio Processing:** It processes audio blobs using Web Audio API, transcribes them, and handles responses from an LLM.
- **Error Handling:** Errors are logged and handled gracefully, ensuring the system returns to an idle state after errors.

**When/Why to Use:**
Use this hook in applications requiring voice interaction with a 3D environment. It's ideal for scenarios where real-time transcription and response handling are needed, such as virtual assistants or interactive voice interfaces.

**Patterns/Gotchas:**
- **AbortController:** The `abortControllerRef` is used to gracefully handle interruptions during LLM processing.
- **State Updates:** Ensure that state updates (e.g., `updateStatus`) are called appropriately to maintain the correct flow of the pipeline.
- **External Dependencies:** This function relies on external components like `transcribeAudio`, `streamChat`, and `synthesizeSpeech`. Ensure these dependencies are properly implemented.

---

*Generated by CodeWorm on 2026-02-27 21:09*
