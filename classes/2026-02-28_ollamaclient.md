# OllamaClient

**Type:** Class Documentation
**Repository:** CodeWorm
**File:** codeworm/llm/client.py
**Language:** python
**Lines:** 63-295
**Complexity:** 0.0

---

## Source Code

```python
class OllamaClient:
    """
    Async client for Ollama API
    Handles connection pooling, retries, and OOM recovery
    """
    DEFAULT_TIMEOUT = httpx.Timeout(timeout = 600.0, connect = 10.0)

    def __init__(self, settings: OllamaSettings) -> None:
        """
        Initialize client with settings
        """
        self.settings = settings
        self.base_url = settings.base_url
        self._client: httpx.AsyncClient | None = None
        self._model_loaded = False

    async def _get_client(self) -> httpx.AsyncClient:
        """
        Get or create the HTTP client
        """
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url = self.base_url,
                timeout = self.DEFAULT_TIMEOUT,
                limits = httpx.Limits(
                    max_keepalive_connections = 20,
                    max_connections = 50,
                    keepalive_expiry = 60.0,
                ),
            )
        return self._client

    async def close(self) -> None:
        """
        Close the HTTP client
        """
        if self._client:
            await self._client.aclose()
            self._client = None

    async def health_check(self) -> bool:
        """
        Check if Ollama is running and responsive
        """
        try:
            client = await self._get_client()
            response = await client.get("/", timeout = 5.0)
            return response.status_code == 200
        except Exception:
            return False

    async def prewarm(self) -> bool:
        """
        Load model into memory and keep it warm
        """
        try:
            client = await self._get_client()
            response = await client.post(
                "/api/generate",
                json = {
                    "model": self.settings.model,
                    "prompt": "",
                    "keep_alive": self.settings.keep_alive,
                    "options": {
                        "num_ctx": self.settings.num_ctx,
                    },
                },
            )

            if response.status_code == 200:
                self._model_loaded = True
                logger.info(
                    "model_prewarmed",
                    model = self.settings.model,
                    num_ctx = self.settings.num_ctx,
                )
                return True

            return False

        except Exception as e:
            logger.error("prewarm_failed", error = str(e))
            return False

    async def generate(
        self,
        prompt: str,
        system: str | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> GenerationResult:
        """
        Generate text from a prompt
        """
        client = await self._get_client()

        options = {
            "temperature": temperature or self.settings.temperature,
            "num_predict": max_tokens or self.settings.num_predict,
    
```

---

## Class Documentation

### OllamaClient Documentation

**Class Responsibility and Purpose:**
The `OllamaClient` class serves as an asynchronous client for interacting with the Ollama API. It handles connection pooling, retries, and out-of-memory (OOM) recovery to ensure robust communication with the Ollama server.

**Public Interface:**
- **Initialization (`__init__`):** Initializes the client with settings.
- **Prewarming (`prewarm`):** Loads the model into memory for faster subsequent requests.
- **Health Check (`health_check`):** Verifies if the Ollama server is running and responsive.
- **Generate Text (`generate`):** Generates text from a given prompt using the specified model.

**Design Patterns Used:**
- **Factory Pattern:** Implicitly used in creating the `httpx.AsyncClient` instance within `_get_client`.
- **Strategy Pattern:** The `settings` object acts as a strategy for configuring client behavior.
- **Observer Pattern:** Logging and error handling can be seen as an observer pattern, where errors are logged.

**Relationship to Other Classes:**
- **Dependent on `OllamaSettings`:** Uses the settings provided by `OllamaSettings` to configure its behavior.
- **Interacts with `httpx.AsyncClient`:** Manages and reuses an HTTP client instance for making API calls, ensuring efficient resource management.

This class is a key component in managing interactions with the Ollama API, providing a robust interface for generating text while handling common issues like connection errors and model out-of-memory conditions.

---

*Generated by CodeWorm on 2026-02-28 14:35*
