# OllamaClient

**Type:** Class Documentation
**Repository:** CodeWorm
**File:** codeworm/llm/client.py
**Language:** python
**Lines:** 63-295
**Complexity:** 0.0

---

## Source Code

```python
class OllamaClient:
    """
    Async client for Ollama API
    Handles connection pooling, retries, and OOM recovery
    """
    DEFAULT_TIMEOUT = httpx.Timeout(timeout = 600.0, connect = 10.0)

    def __init__(self, settings: OllamaSettings) -> None:
        """
        Initialize client with settings
        """
        self.settings = settings
        self.base_url = settings.base_url
        self._client: httpx.AsyncClient | None = None
        self._model_loaded = False

    async def _get_client(self) -> httpx.AsyncClient:
        """
        Get or create the HTTP client
        """
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url = self.base_url,
                timeout = self.DEFAULT_TIMEOUT,
                limits = httpx.Limits(
                    max_keepalive_connections = 20,
                    max_connections = 50,
                    keepalive_expiry = 60.0,
                ),
            )
        return self._client

    async def close(self) -> None:
        """
        Close the HTTP client
        """
        if self._client:
            await self._client.aclose()
            self._client = None

    async def health_check(self) -> bool:
        """
        Check if Ollama is running and responsive
        """
        try:
            client = await self._get_client()
            response = await client.get("/")
            return response.status_code == 200
        except Exception:
            return False

    async def prewarm(self) -> bool:
        """
        Load model into memory and keep it warm
        """
        try:
            client = await self._get_client()
            response = await client.post(
                "/api/generate",
                json = {
                    "model": self.settings.model,
                    "prompt": "",
                    "keep_alive": self.settings.keep_alive,
                    "options": {
                        "num_ctx": self.settings.num_ctx,
                    },
                },
            )

            if response.status_code == 200:
                self._model_loaded = True
                logger.info(
                    "model_prewarmed",
                    model = self.settings.model,
                    num_ctx = self.settings.num_ctx,
                )
                return True

            return False

        except Exception as e:
            logger.error("prewarm_failed", error = str(e))
            return False

    async def generate(
        self,
        prompt: str,
        system: str | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> GenerationResult:
        """
        Generate text from a prompt
        """
        client = await self._get_client()

        options = {
            "temperature": temperature or self.settings.temperature,
            "num_predict": max_tokens or self.settings.num_predict,
            "num_ct
```

---

## Class Documentation

### OllamaClient Documentation

**Class Responsibility and Purpose:**
The `OllamaClient` class is an asynchronous client designed to interact with the Ollama API. It handles connection pooling, retries, and out-of-memory (OOM) recovery mechanisms to ensure robust communication with the API.

**Public Interface:**
- **Initialization (`__init__`):** Initializes the client with settings.
- **Health Check (`health_check`):** Checks if the Ollama server is running and responsive.
- **Prewarm (`prewarm`):** Loads the model into memory to keep it warm for faster subsequent requests.
- **Generate (`generate`):** Generates text from a given prompt using the specified model.

**Design Patterns Used:**
- **Singleton Pattern:** The `_client` attribute ensures that only one `httpx.AsyncClient` instance is created and reused, adhering to the singleton pattern.
- **Error Handling:** Comprehensive error handling with specific exceptions like `OllamaModelError`, `OllamaConnectionError`, and `OllamaTimeoutError`.

**Relationship to Other Classes:**
- The class interacts with `OllamaSettings` for configuration settings and `GenerationResult` for storing the results of text generation.
- It is part of a larger architecture where it acts as an intermediary between the application logic and the Ollama API.

This class plays a crucial role in managing asynchronous HTTP requests, ensuring efficient and reliable communication with the Ollama API.

---

*Generated by CodeWorm on 2026-02-19 13:15*
