# ComplexityAnalyzer.analyze_directory

**Type:** Performance Analysis
**Repository:** CodeWorm
**File:** codeworm/analysis/complexity.py
**Language:** python
**Lines:** 165-183
**Complexity:** 6.0

---

## Source Code

```python
def analyze_directory(self,
                          directory: Path,
                          recursive: bool = True) -> Iterator[FileComplexity]:
        """
        Analyze all supported files in a directory
        """
        pattern = "**/*" if recursive else "*"
        for ext in self._extensions:
            for file_path in directory.glob(f"{pattern}{ext}"):
                if file_path.is_file():
                    try:
                        yield self.analyze_file(file_path)
                    except Exception as e:
                        logger.debug(
                            "file_analysis_failed",
                            file = str(file_path),
                            error = str(e)
                        )
                        continue
```

---

## Performance Analysis

### Performance Analysis

#### Time Complexity
The time complexity of the `analyze_directory` function is \(O(n \cdot m)\), where \(n\) is the number of files in the directory and its subdirectories (if `recursive` is `True`), and \(m\) is the average time taken to analyze each file. This is because the function iterates over all matching files, and for each file, it calls `analyze_file`.

#### Space Complexity
The space complexity is primarily influenced by the number of files being processed and the stack depth if recursion is used (`recursive=True`). The primary memory usage comes from storing the list of file paths generated by `directory.glob()`.

#### Bottlenecks or Inefficiencies
1. **Redundant Iteration**: The function iterates over all matching files, even those that fail analysis.
2. **Exception Handling Overhead**: Catching and logging exceptions for each file can be costly if many files fail to analyze.
3. **Unclosed Resources**: No context managers are used to ensure resources like loggers or file handles are properly managed.

#### Optimization Opportunities
1. **Filter Out Failed Files Early**: Instead of catching exceptions, consider filtering out non-analyzable files early using a try-except block around the `directory.glob()` call.
2. **Use Context Managers for Logging**: Ensure that logging is efficiently handled by using context managers or other mechanisms to avoid redundant setup and teardown.
3. **Optimize File Analysis**: If `analyze_file` is costly, consider caching results if files are reanalyzed frequently.

#### Resource Usage Concerns
- **Unclosed Connections/Handles**: Ensure all resources used within the function are properly closed, especially in async contexts where unclosed handles can lead to resource leaks.
- **Logging Overhead**: Minimize logging overhead by batching log entries or using more efficient logging mechanisms.

By addressing these points, you can improve both the performance and robustness of the `analyze_directory` function.

---

*Generated by CodeWorm on 2026-02-22 09:25*
