# GitHubService.get_user_repositories

**Type:** Performance Analysis
**Repository:** kill-pr0cess.inc
**File:** backend/src/services/github_service.rs
**Language:** rust
**Lines:** 125-200
**Complexity:** 10.0

---

## Source Code

```rust
pub async fn get_user_repositories(&self, username: &str) -> Result<Vec<Repository>> {
        let cache_key = format!("github:repos:{}", username);

        // Check cache first - I'm implementing intelligent cache with TTL
        if let Ok(Some(cached_repos)) = self.cache_service.get::<Vec<Repository>>(&cache_key).await {
            debug!("Returning cached repositories for user: {}", username);
            return Ok(cached_repos);
        }

        info!("Fetching fresh repository data for user: {}", username);

        let mut all_repos = Vec::new();
        let mut page = 1;
        let per_page = 100; // Maximum allowed by GitHub API

        loop {
            // I'm checking rate limits before making requests
            self.check_rate_limit().await?;

            let url = format!(
                "{}/users/{}/repos?page={}&per_page={}&sort=updated&direction=desc",
                self.base_url, username, page, per_page
            );

            debug!("Fetching repositories page {} for user: {}", page, username);

            let response = self.client
            .get(&url)
            .send()
            .await
            .map_err(|e| AppError::ExternalApiError(format!("GitHub API request failed: {}", e)))?;

            // Update rate limit information from headers
            self.update_rate_limit_from_headers(&response).await;

            if !response.status().is_success() {
                let status = response.status();
                let error_text = response.text().await.unwrap_or_default();
                return Err(AppError::ExternalApiError(
                    format!("GitHub API error {}: {}", status, error_text)
                ));
            }

            let repos: Vec<GitHubApiRepository> = response
            .json()
            .await
            .map_err(|e| AppError::SerializationError(format!("Failed to parse GitHub response: {}", e)))?;

            if repos.is_empty() {
                break; // No more pages
            }

            // Transform GitHub API response to our internal format
            for api_repo in repos {
                let repo = self.transform_api_repository(api_repo);
                all_repos.push(repo);
            }

            page += 1;

            // Prevent infinite loops and respect API limits
            if page > 50 {
                warn!("Stopping repository fetch at page 50 to prevent excessive API usage");
                break;
            }
        }

        info!("Fetched {} repositories for user: {}", all_repos.len(), username);

        // Cache the results with 1-hour TTL
          if let Err(e) = self.cache_service.set(&cache_key, &all_repos, Some(3600)).await {
            warn!("Failed to cache repository data: {}", e);
        }

        Ok(all_repos)
    }
```

---

## Performance Analysis

### Performance Analysis

**Time Complexity**: The time complexity is primarily driven by the number of API requests made, which could be up to 50 pages (100 repositories per page). Each request involves network I/O and JSON deserialization, making it `O(n)` where `n` is the total number of repositories.

**Space Complexity**: The space complexity is `O(n)` due to storing all fetched repositories in memory before caching. This could be optimized by processing and returning results incrementally.

### Bottlenecks or Inefficiencies

1. **Redundant API Requests**: Fetching up to 50 pages might not always be necessary, especially if the user has fewer repositories.
2. **Rate Limit Checks**: Rate limit checks are performed on every page, which could be optimized by checking once and handling retries.
3. **Infinite Loop Guard**: The `page > 50` check is a crude guard against infinite loops but might not handle all edge cases.

### Optimization Opportunities

1. **Incremental Processing**: Process and return repositories incrementally to reduce memory usage.
2. **Rate Limit Handling**: Implement exponential backoff for rate limit retries instead of checking on every request.
3. **Optimize Cache Key**: Ensure cache keys are unique and efficient, possibly using a more granular key structure.

### Resource Usage Concerns

1. **Unclosed Connections**: Ensure that all HTTP connections are properly closed to avoid resource leaks.
2. **Error Handling**: Improve error handling by logging detailed errors and providing better user feedback.

By addressing these points, you can enhance the efficiency and robustness of your code.

---

*Generated by CodeWorm on 2026-02-21 19:09*
