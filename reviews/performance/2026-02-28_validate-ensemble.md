# validate_ensemble

**Type:** Performance Analysis
**Repository:** Cybersecurity-Projects
**File:** PROJECTS/advanced/ai-threat-detection/backend/ml/validation.py
**Language:** python
**Lines:** 54-113
**Complexity:** 5.0

---

## Source Code

```python
def validate_ensemble(
    model_dir: Path,
    X_test: np.ndarray,
    y_test: np.ndarray,
    ensemble_weights: dict[str, float] | None = None,
    pr_auc_gate: float = 0.85,
    f1_gate: float = 0.80,
) -> ValidationResult:
    """
    Run all 3 models on test data and compute classification metrics
    """
    weights = ensemble_weights or DEFAULT_ENSEMBLE_WEIGHTS

    engine = InferenceEngine(model_dir=str(model_dir))
    if not engine.is_loaded:
        raise RuntimeError(f"Failed to load models from {model_dir}")

    raw_scores = engine.predict(X_test.astype(np.float32), )
    if raw_scores is None:
        raise RuntimeError("Inference returned None")

    fused = _compute_fused_scores(raw_scores, engine.threshold, weights)

    y_pred = (fused >= BINARY_THRESHOLD).astype(np.int32)

    prec = float(precision_score(y_test, y_pred, zero_division=0))
    rec = float(recall_score(y_test, y_pred, zero_division=0))
    f1_val = float(f1_score(y_test, y_pred, zero_division=0))
    pr_auc_val = float(average_precision_score(y_test, fused))
    roc_auc_val = float(roc_auc_score(y_test, fused))

    cm = confusion_matrix(y_test, y_pred).tolist()

    pr_auc_passed = pr_auc_val >= pr_auc_gate
    f1_passed = f1_val >= f1_gate
    gate_details = {
        "pr_auc": pr_auc_passed,
        "f1": f1_passed,
    }

    logger.info(
        "Validation: precision=%.3f recall=%.3f "
        "f1=%.3f pr_auc=%.3f roc_auc=%.3f",
        prec,
        rec,
        f1_val,
        pr_auc_val,
        roc_auc_val,
    )

    return ValidationResult(
        precision=prec,
        recall=rec,
        f1=f1_val,
        pr_auc=pr_auc_val,
        roc_auc=roc_auc_val,
        confusion_matrix=cm,
        passed_gates=pr_auc_passed and f1_passed,
        gate_details=gate_details,
    )
```

---

## Performance Analysis

### Performance Analysis

#### Time Complexity
The time complexity is dominated by the `engine.predict` call, which likely involves model inference. Assuming this operation takes \(O(n \cdot m)\) where \(n\) is the number of samples and \(m\) is the feature count, the overall complexity is \(O(n \cdot m + k)\), with \(k\) representing additional operations.

#### Space Complexity
The primary space usage comes from storing `raw_scores` and `fused`, which require \(O(n \cdot p)\) space, where \(p\) is the number of models. Other variables like `y_pred`, `prec`, `rec`, etc., are minimal.

#### Bottlenecks or Inefficiencies
1. **Redundant Type Conversions**: Converting `X_test` to `np.float32` and then casting `fused` to `np.int32` is unnecessary if the input data type matches.
2. **Multiple Score Calculations**: Computing precision, recall, F1, PR AUC, and ROC AUC multiple times can be optimized by calculating them once and storing intermediate results.

#### Optimization Opportunities
1. **Cache Model Inference Results**: If `engine.predict` is expensive, cache the results using a context manager or LRU cache.
2. **Use Vectorized Operations**: Ensure that all operations are vectorized to leverage NumPyâ€™s efficient computation capabilities.
3. **Reduce Redundant Checks**: Simplify error handling and redundant checks.

#### Resource Usage Concerns
- Ensure `engine` is properly closed if it implements a context manager or has a `.close()` method.
- Use context managers for file handles and connections where applicable to avoid leaks.

By addressing these points, you can enhance the efficiency of your validation function.

---

*Generated by CodeWorm on 2026-02-28 09:37*
