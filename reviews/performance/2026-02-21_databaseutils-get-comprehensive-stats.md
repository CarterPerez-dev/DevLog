# DatabaseUtils.get_comprehensive_stats

**Type:** Performance Analysis
**Repository:** kill-pr0cess.inc
**File:** backend/src/database/mod.rs
**Language:** rust
**Lines:** 105-194
**Complexity:** 9.0

---

## Source Code

```rust
pub async fn get_comprehensive_stats(pool: &DatabasePool) -> Result<serde_json::Value> {
        // Table sizes
        let table_sizes = sqlx::query(
            "SELECT
                schemaname,
                tablename,
                pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
                pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
            FROM pg_tables
            WHERE schemaname = 'public'
            ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC"
        )
        .fetch_all(pool)
        .await?;

        // Connection stats
        let connection_stats = sqlx::query(
            "SELECT
                count(*) as total_connections,
                count(*) FILTER (WHERE state = 'active') as active_connections,
                count(*) FILTER (WHERE state = 'idle') as idle_connections
            FROM pg_stat_activity"
        )
        .fetch_one(pool)
        .await?;

        // Database stats
        let db_stats = sqlx::query(
            "SELECT
                numbackends,
                xact_commit,
                xact_rollback,
                blks_read,
                blks_hit,
                tup_returned,
                tup_fetched,
                tup_inserted,
                tup_updated,
                tup_deleted
            FROM pg_stat_database
            WHERE datname = current_database()"
        )
        .fetch_one(pool)
        .await?;

        // Pre-compute block read/hit statistics for hit ratio
        let blks_read: i64 = db_stats.try_get("blks_read")?;
        let blks_hit: i64 = db_stats.try_get("blks_hit")?;
        let hit_ratio = if blks_read + blks_hit > 0 {
            blks_hit as f64 / (blks_read + blks_hit) as f64 * 100.0
        } else {
            0.0
        };

        let stats = serde_json::json!({
            "table_sizes": table_sizes.iter().map(|row| {
                serde_json::json!({
                    "table": row.get::<String, _>("tablename"),
                    "size": row.get::<String, _>("size"),
                    "size_bytes": row.get::<i64, _>("size_bytes")
                })
            }).collect::<Vec<_>>(),
            "connections": {
                "total": connection_stats.get::<i64, _>("total_connections"),
                "active": connection_stats.get::<i64, _>("active_connections"),
                "idle": connection_stats.get::<i64, _>("idle_connections")
            },
            "database": {
                "backends": db_stats.try_get::<i32, _>("numbackends")?,
                "transactions": {
                    "committed": db_stats.try_get::<i64, _>("xact_commit")?,
                    "rolled_back": db_stats.get::<i64, _>("xact_rollback")
                },
                    "blocks": {
                        "read": blks_read,
                        "hit": blks_hit,
                        "hit_ratio": hit_ratio
                    },
                "tuples
```

---

## Performance Analysis

### Performance Analysis

**Time Complexity**: The function has a time complexity of \(O(n + m)\), where \(n\) is the number of tables in the `public` schema and \(m\) is the number of connections currently active or idle. This is due to the SQL queries fetching data from multiple tables.

**Space Complexity**: The space complexity is primarily determined by the memory required to store the fetched rows, which scales with the number of tables and connections. The JSON serialization adds a small constant overhead.

**Bottlenecks/Inefficiencies**:
1. **Multiple Queries**: Executing three separate SQL queries can be inefficient if each query fetches a large amount of data.
2. **Redundant Operations**: Calculating `hit_ratio` involves multiple calls to `try_get`, which could be optimized by fetching all necessary values in one go.

**Optimization Opportunities**:
1. **Combine Queries**: Use a single SQL query to fetch table sizes and connection stats, reducing the number of database roundtrips.
2. **Batch Fetching**: If possible, batch the results from `pg_stat_activity` and `pg_stat_database` to reduce I/O operations.

**Resource Usage Concerns**:
- Ensure that the `DatabasePool` is properly managed to avoid resource leaks.
- Use `try_get` judiciously to handle potential errors gracefully without unnecessary iterations.

By optimizing the SQL queries and reducing the number of database calls, you can improve both performance and resource utilization.

---

*Generated by CodeWorm on 2026-02-21 19:01*
