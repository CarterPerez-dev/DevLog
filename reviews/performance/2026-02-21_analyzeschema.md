# AnalyzeSchema

**Type:** Performance Analysis
**Repository:** angelamos-operations
**File:** CertGamesDB-Argos/go-backend/internal/mongodb/collections.go
**Language:** go
**Lines:** 208-281
**Complexity:** 10.0

---

## Source Code

```go
func (r *CollectionsRepository) AnalyzeSchema(ctx context.Context, dbName, collName string, sampleSize int) (*SchemaAnalysis, error) {
	db := r.client.client.Database(dbName)
	coll := db.Collection(collName)

	totalDocs, err := coll.EstimatedDocumentCount(ctx)
	if err != nil {
		return nil, fmt.Errorf("count documents: %w", err)
	}

	if sampleSize <= 0 {
		sampleSize = 1000
	}
	if int64(sampleSize) > totalDocs {
		sampleSize = int(totalDocs)
	}

	pipeline := mongo.Pipeline{
		{{Key: "$sample", Value: bson.D{{Key: "size", Value: sampleSize}}}},
	}

	cursor, err := coll.Aggregate(ctx, pipeline)
	if err != nil {
		return nil, fmt.Errorf("sample documents: %w", err)
	}
	defer cursor.Close(ctx)

	fieldMap := make(map[string]*fieldInfo)
	var sampledCount int64

	for cursor.Next(ctx) {
		var doc bson.M
		if err := cursor.Decode(&doc); err != nil {
			continue
		}
		sampledCount++
		analyzeDocument("", doc, fieldMap)
	}

	var fields []FieldSchema
	for name, info := range fieldMap {
		typeList := make([]string, 0, len(info.types))
		for t := range info.types {
			typeList = append(typeList, t)
		}
		sort.Strings(typeList)

		coverage := float64(info.count) / float64(sampledCount) * 100

		samples := info.samples
		if len(samples) > 5 {
			samples = samples[:5]
		}

		fields = append(fields, FieldSchema{
			Name:         name,
			Types:        typeList,
			Coverage:     coverage,
			Count:        info.count,
			TotalDocs:    sampledCount,
			SampleValues: samples,
		})
	}

	sort.Slice(fields, func(i, j int) bool {
		return fields[i].Coverage > fields[j].Coverage
	})

	return &SchemaAnalysis{
		CollectionName: collName,
		TotalDocuments: totalDocs,
		SampleSize:     sampledCount,
		Fields:         fields,
	}, nil
}
```

---

## Performance Analysis

### Performance Analysis

**Time Complexity:** O(n log n + m * k), where `n` is the number of fields, `m` is the sample size, and `k` is the number of unique types per field due to sorting.

**Space Complexity:** O(m + n), for storing sampled documents and field metadata.

**Bottlenecks or Inefficiencies:**
1. **Sorting Operations:** Sorting the type list within each field can be costly.
2. **Redundant Iterations:** The `analyzeDocument` function is called with an empty string, which might not be necessary if it's a placeholder.
3. **Unnecessary Closures:** The `cursor.Close(ctx)` is deferred but could be optimized by closing immediately after use.

**Optimization Opportunities:**
1. **Remove Unnecessary Sorting:** If the types don't need to be sorted, remove the sorting operation to reduce time complexity.
2. **Pass Context Properly:** Ensure `analyzeDocument` handles context correctly or avoid calling it if not needed.
3. **Immediate Cursor Closure:** Close the cursor immediately after use instead of deferring.

**Resource Usage Concerns:**
- Ensure all database connections are properly managed and closed.
- Use context cancellation to handle early exits from loops.

### Suggested Optimizations
1. Remove sorting in `typeList` if not required.
2. Pass a proper context to `analyzeDocument` or remove the call if unnecessary.
3. Close cursor immediately after use: `if err := cursor.Close(ctx); err != nil { return fmt.Errorf("close cursor: %w", err) }`.

---

*Generated by CodeWorm on 2026-02-21 17:42*
