# scanStaged

**Type:** Performance Analysis
**Repository:** Cybersecurity-Projects
**File:** PROJECTS/intermediate/secrets-scanner/internal/source/git.go
**Language:** go
**Lines:** 73-135
**Complexity:** 16.0

---

## Source Code

```go
func (g *Git) scanStaged(
	ctx context.Context,
	repo *git.Repository,
	out chan<- types.Chunk,
) error {
	wt, err := repo.Worktree()
	if err != nil {
		return fmt.Errorf("worktree: %w", err)
	}

	status, err := wt.Status()
	if err != nil {
		return fmt.Errorf("status: %w", err)
	}

	idx, err := repo.Storer.Index()
	if err != nil {
		return fmt.Errorf("index: %w", err)
	}

	for _, entry := range idx.Entries {
		if ctx.Err() != nil {
			return ctx.Err()
		}

		fileStatus := status.File(entry.Name)
		if fileStatus.Staging == git.Unmodified &&
			fileStatus.Worktree == git.Unmodified {
			continue
		}

		if g.isExcluded(entry.Name) || isBinaryExt(entry.Name) {
			continue
		}

		blob, blobErr := repo.BlobObject(entry.Hash)
		if blobErr != nil {
			continue
		}

		if blob.Size > g.MaxSize {
			continue
		}

		content, readErr := readBlob(blob)
		if readErr != nil {
			continue
		}

		chunks := splitIntoChunks(
			content, entry.Name, "", "", time.Time{},
		)
		for _, chunk := range chunks {
			select {
			case <-ctx.Done():
				return ctx.Err()
			case out <- chunk:
			}
		}
	}

	return nil
}
```

---

## Performance Analysis

### Performance Analysis

**Time Complexity:** The function has a time complexity of \(O(n \times m)\), where \(n\) is the number of files in the index, and \(m\) is the average size of each file divided by `g.MaxSize`. This is due to iterating over all indexed entries and potentially reading large blobs.

**Space Complexity:** The space complexity is primarily determined by the `chunks` slice. If many chunks are generated for large files, this can consume significant memory.

**Bottlenecks or Inefficiencies:**
- **Redundant Operations:** The function checks if a file is staged multiple times (e.g., in both `wt.Status()` and `fileStatus.Staging`). This can be optimized by using the status directly.
- **Blocking Calls:** Reading blobs (`readBlob(blob)`) may block, especially for large files. Consider using non-blocking reads or streaming.

**Optimization Opportunities:**
- Use a more efficient method to check file exclusions and binary statuses.
- Implement chunked reading of blobs to avoid blocking on large files.
- Cache `repo.BlobObject(entry.Hash)` results if the same entries are scanned frequently.

**Resource Usage Concerns:**
- Ensure proper error handling and resource management, especially for file handles in `readBlob`.
- Close any open connections or file handles that might be used internally by `git.Repository`.

By addressing these points, you can improve both performance and resource efficiency.

---

*Generated by CodeWorm on 2026-02-20 07:54*
