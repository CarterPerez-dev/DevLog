# ollama.client

**Type:** File Overview
**Repository:** angelamos-3d
**File:** frontend/src/api/ollama.client.ts
**Language:** typescript
**Lines:** 1-75
**Complexity:** 0.0

---

## Source Code

```typescript
// ===================
// Â© AngelaMos | 2026
// ollama.client.ts
// ===================

import { getAngelaConfig } from "../config";
import type { OllamaMessage, OllamaStreamChunk } from "../types";

export async function* streamChat(
	messages: OllamaMessage[],
	signal?: AbortSignal,
): AsyncGenerator<string, string, unknown> {
	const config = getAngelaConfig();

	const response = await fetch(`${config.api.baseUrl}/v1/chat`, {
		method: "POST",
		headers: { "Content-Type": "application/json" },
		body: JSON.stringify({
			messages,
			stream: true,
		}),
		signal,
	});

	if (!response.ok) {
		throw new Error(`Chat request failed: ${response.status}`);
	}

	const reader = response.body?.getReader();
	if (!reader) {
		throw new Error("No response body");
	}

	const decoder = new TextDecoder();
	let fullResponse = "";

	while (true) {
		const { done, value } = await reader.read();
		if (done) break;

		const chunk = decoder.decode(value, { stream: true });
		const lines = chunk.split("\n").filter((line) => line.trim());

		for (const line of lines) {
			try {
				const data: OllamaStreamChunk = JSON.parse(line);
				if (data.message?.content) {
					fullResponse += data.message.content;
					yield data.message.content;
				}
			} catch {}
		}
	}

	return fullResponse;
}

export async function chat(messages: OllamaMessage[]): Promise<string> {
	let result = "";
	for await (const chunk of streamChat(messages)) {
		result += chunk;
	}
	return result;
}

export async function checkHealth(): Promise<boolean> {
	const config = getAngelaConfig();
	try {
		const response = await fetch(`${config.api.baseUrl}/health`);
		return response.ok;
	} catch {
		return false;
	}
}

```

---

## File Overview

# ollama.client.ts Documentation

**Purpose:** This file provides client-side functionality for interacting with an Ollama API, specifically handling chat requests and health checks.

**Key Exports:**
- `streamChat`: An asynchronous generator function that streams chat responses from the server.
- `chat`: A promise-based wrapper around `streamChat` to simplify usage.
- `checkHealth`: A utility function to check the API's health status.

**Project Fit:** This file is part of the frontend API layer, responsible for communicating with backend services. It integrates with configuration management and type definitions from other modules within the project.

**Design Decisions:**
1. **Asynchronous Generators**: Used in `streamChat` to handle streaming responses efficiently.
2. **Error Handling**: Robust error handling ensures that failed requests are properly managed, providing clear feedback through exceptions.
3. **Dependency Injection**: The `AbortSignal` parameter allows for graceful cancellation of ongoing fetch operations.
4. **Type Safety**: Utilizes TypeScript types and generics to ensure type safety across the API functions.

This file is crucial for maintaining seamless communication between the frontend and backend services, ensuring that chat interactions are smooth and reliable.

---

*Generated by CodeWorm on 2026-02-21 00:31*
