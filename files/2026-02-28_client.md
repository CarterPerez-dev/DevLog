# client

**Type:** File Overview
**Repository:** CodeWorm
**File:** codeworm/llm/client.py
**Language:** python
**Lines:** 1-308
**Complexity:** 0.0

---

## Source Code

```python
"""
â’¸AngelaMos | 2026
llm/client.py
"""
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import TYPE_CHECKING

import httpx

from codeworm.core import get_logger

if TYPE_CHECKING:
    from codeworm.core.config import OllamaSettings


logger = get_logger("llm")


class OllamaError(Exception):
    """
    Base exception for Ollama errors
    """


class OllamaConnectionError(OllamaError):
    """
    Failed to connect to Ollama
    """


class OllamaModelError(OllamaError):
    """
    Model-related error like OOM
    """


class OllamaTimeoutError(OllamaError):
    """
    Request timed out
    """


@dataclass
class GenerationResult:
    """
    Result from an LLM generation request
    """
    text: str
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_duration_ms: int
    tokens_per_second: float

    @property
    def total_tokens(self) -> int:
        return self.prompt_tokens + self.completion_tokens


class OllamaClient:
    """
    Async client for Ollama API
    Handles connection pooling, retries, and OOM recovery
    """
    DEFAULT_TIMEOUT = httpx.Timeout(timeout = 600.0, connect = 10.0)

    def __init__(self, settings: OllamaSettings) -> None:
        """
        Initialize client with settings
        """
        self.settings = settings
        self.base_url = settings.base_url
        self._client: httpx.AsyncClient | None = None
        self._model_loaded = False

    async def _get_client(self) -> httpx.AsyncClient:
        """
        Get or create the HTTP client
        """
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url = self.base_url,
                timeout = self.DEFAULT_TIMEOUT,
                limits = httpx.Limits(
                    max_keepalive_connections = 20,
                    max_connections = 50,
                    keepalive_expiry = 60.0,
                ),
            )
        return self._client

    async def close(self) -> None:
        """
        Close the HTTP client
        """
        if self._client:
            await self._client.aclose()
            self._client = None

    async def health_check(self) -> bool:
        """
        Check if Ollama is running and responsive
        """
        try:
            client = await self._get_client()
            response = await client.get("/", timeout = 5.0)
            return response.status_code == 200
        except Exception:
            return False

    async def prewarm(self) -> bool:
        """
        Load model into memory and keep it warm
        """
        try:
            client = await self._get_client()
            response = await client.post(
                "/api/generate",
                json = {
                    "model": self.settings.model,
                    "prompt": "",
                    "keep_alive": self.settings.keep_alive,
                    "options": {
                        "num_ct
```

---

## File Overview

**File Purpose and Responsibility:**
This Python source file defines an asynchronous client for interacting with the Ollama API, responsible for handling model loading, generation requests, and health checks.

**Key Exports or Public Interface:**
- `OllamaClient`: An async client class that manages connection pooling, retries, and OOM recovery.
- `GenerationResult`: A dataclass representing the result of an LLM generation request.
- Various error classes (`OllamaError`, `OllamaConnectionError`, `OllamaModelError`, `OllamaTimeoutError`).

**How It Fits in the Project:**
This file is part of the `codeworm.llm` module, which provides a high-level interface for interacting with language models. The `OllamaClient` class is used by other components within the project to make API calls and manage model resources.

**Notable Design Decisions:**
- Use of `httpx.AsyncClient` for handling HTTP requests asynchronously.
- Implementation of connection pooling and retries through `httpx.Limits`.
- Custom error classes to handle specific Ollama-related issues.
- Asynchronous methods (`_get_client`, `health_check`, `prewarm`, `generate`) to ensure non-blocking operations.

---

*Generated by CodeWorm on 2026-02-28 15:20*
