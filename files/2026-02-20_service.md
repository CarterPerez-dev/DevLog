# service

**Type:** File Overview
**Repository:** angelamos-3d
**File:** backend/app/chat/service.py
**Language:** python
**Lines:** 1-78
**Complexity:** 0.0

---

## Source Code

```python
"""
Â©AngelaMos | 2026
service.py
"""

from collections.abc import AsyncGenerator
from typing import Any

import httpx

from app.chat.schemas import ChatResponse, Message
from app.config import ANGELA_SYSTEM_PROMPT, settings
from app.core.exceptions import OllamaConnectionError


SYSTEM_MESSAGE = Message(role = "system", content = ANGELA_SYSTEM_PROMPT)


def build_payload(messages: list[Message], stream: bool) -> dict[str, Any]:
    """
    Build Ollama request payload with system prompt and options.
    """
    all_messages = [SYSTEM_MESSAGE, *messages]
    return {
        "model": settings.OLLAMA_MODEL,
        "messages": [m.model_dump() for m in all_messages],
        "stream": stream,
        "options": {
            "temperature": settings.OLLAMA_TEMPERATURE,
            "num_predict": settings.OLLAMA_MAX_TOKENS,
        },
    }


async def stream_chat(messages: list[Message]) -> AsyncGenerator[bytes, None]:
    """
    Stream chat responses from Ollama.
    """
    payload = build_payload(messages, stream = True)

    try:
        async with httpx.AsyncClient() as client:  # noqa: SIM117
            async with client.stream(
                    "POST",
                    f"{settings.OLLAMA_HOST}/api/chat",
                    json = payload,
                    timeout = settings.OLLAMA_TIMEOUT,
            ) as response:
                async for line in response.aiter_lines():
                    if line:
                        yield line.encode() + b"\n"
    except httpx.ConnectError as e:
        raise OllamaConnectionError() from e


async def chat(messages: list[Message]) -> ChatResponse:
    """
    Send chat request to Ollama and return complete response.
    """
    payload = build_payload(messages, stream = False)

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{settings.OLLAMA_HOST}/api/chat",
                json = payload,
                timeout = settings.OLLAMA_TIMEOUT,
            )
            data = response.json()

        return ChatResponse(
            message = Message(**data["message"]),
            model = data["model"],
            done = data["done"],
        )
    except httpx.ConnectError as e:
        raise OllamaConnectionError() from e

```

---

## File Overview

# service.py

**Purpose**: This file provides core functionality for interacting with an Ollama chat API, handling both streaming and non-streaming requests.

**Key Exports**:
- `build_payload`: Constructs the request payload for chat interactions.
- `stream_chat`: Streams chat responses from Ollama in real-time.
- `chat`: Sends a complete chat request to Ollama and returns the response.

**Project Integration**: 
This file is part of the `app/chat` module, which handles all chat-related operations. It interacts with the `httpx` library for HTTP requests and relies on configuration settings from `settings`. The responses are processed using schemas defined in `app.chat.schemas`.

**Design Decisions**:
- **Error Handling**: Uses custom exceptions like `OllamaConnectionError` to handle connection issues.
- **Type Hints & Async Support**: Utilizes Python 3.9+ features such as type hints and asynchronous context managers for efficient, non-blocking operations.
- **Dependencies**: Relies on `httpx`, `typing`, and project-specific schemas and settings.

This file ensures robust communication with the Ollama API while maintaining a clean and modular design within the larger application architecture.

---

*Generated by CodeWorm on 2026-02-20 23:58*
