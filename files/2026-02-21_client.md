# client

**Type:** File Overview
**Repository:** CodeWorm
**File:** codeworm/llm/client.py
**Language:** python
**Lines:** 1-308
**Complexity:** 0.0

---

## Source Code

```python
"""
â’¸AngelaMos | 2026
llm/client.py
"""
from __future__ import annotations

import asyncio
from dataclasses import dataclass
from typing import TYPE_CHECKING

import httpx

from codeworm.core import get_logger

if TYPE_CHECKING:
    from codeworm.core.config import OllamaSettings


logger = get_logger("llm")


class OllamaError(Exception):
    """
    Base exception for Ollama errors
    """


class OllamaConnectionError(OllamaError):
    """
    Failed to connect to Ollama
    """


class OllamaModelError(OllamaError):
    """
    Model-related error like OOM
    """


class OllamaTimeoutError(OllamaError):
    """
    Request timed out
    """


@dataclass
class GenerationResult:
    """
    Result from an LLM generation request
    """
    text: str
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_duration_ms: int
    tokens_per_second: float

    @property
    def total_tokens(self) -> int:
        return self.prompt_tokens + self.completion_tokens


class OllamaClient:
    """
    Async client for Ollama API
    Handles connection pooling, retries, and OOM recovery
    """
    DEFAULT_TIMEOUT = httpx.Timeout(timeout = 600.0, connect = 10.0)

    def __init__(self, settings: OllamaSettings) -> None:
        """
        Initialize client with settings
        """
        self.settings = settings
        self.base_url = settings.base_url
        self._client: httpx.AsyncClient | None = None
        self._model_loaded = False

    async def _get_client(self) -> httpx.AsyncClient:
        """
        Get or create the HTTP client
        """
        if self._client is None:
            self._client = httpx.AsyncClient(
                base_url = self.base_url,
                timeout = self.DEFAULT_TIMEOUT,
                limits = httpx.Limits(
                    max_keepalive_connections = 20,
                    max_connections = 50,
                    keepalive_expiry = 60.0,
                ),
            )
        return self._client

    async def close(self) -> None:
        """
        Close the HTTP client
        """
        if self._client:
            await self._client.aclose()
            self._client = None

    async def health_check(self) -> bool:
        """
        Check if Ollama is running and responsive
        """
        try:
            client = await self._get_client()
            response = await client.get("/")
            return response.status_code == 200
        except Exception:
            return False

    async def prewarm(self) -> bool:
        """
        Load model into memory and keep it warm
        """
        try:
            client = await self._get_client()
            response = await client.post(
                "/api/generate",
                json = {
                    "model": self.settings.model,
                    "prompt": "",
                    "keep_alive": self.settings.keep_alive,
                    "options": {
                        "num_ctx": self.settin
```

---

## File Overview

### llm/client.py

**Purpose**: This file implements an asynchronous client for interacting with the Ollama API, providing methods to manage connections, perform health checks, prewarm models, and generate text.

**Key Exports & Public Interface**:
- `OllamaClient`: An async client class responsible for managing HTTP requests to the Ollama API.
  - Methods: `_get_client`, `close`, `health_check`, `prewarm`, `generate`.
- `GenerationResult`: A dataclass representing the result of an LLM generation request.

**Project Integration**: This file is part of the `codeworm` project, specifically for handling interactions with the Ollama language model. It integrates with other components like configuration settings and logging to ensure robust and reliable API usage.

**Design Decisions & Patterns**:
- **Asynchronous Operations**: Uses `asyncio` and `httpx.AsyncClient` for efficient, non-blocking HTTP requests.
- **Error Handling**: Implements custom exceptions (`OllamaError`, `OllamaConnectionError`, etc.) to handle specific error scenarios.
- **Connection Management**: Manages connection pooling and retries through the `_get_client` method to optimize performance and resource usage.
- **Prewarming**: Ensures models are loaded into memory before generating text, improving response times for subsequent requests.
```

This documentation provides an overview of the file's purpose, key components, integration within the project, and notable design choices.

---

*Generated by CodeWorm on 2026-02-21 07:32*
