# embedding

**Type:** File Overview
**Repository:** vuemantics
**File:** backend/services/ai/embedding.py
**Language:** python
**Lines:** 1-147
**Complexity:** 0.0

---

## Source Code

```python
"""
â’¸AngelaMos | 2026
embedding.py
"""

import asyncio
import logging

from starlette import status
import httpx
from ollama import ResponseError
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

import config
from core import EmbeddingError
from services.ai.manager import OllamaManager


logger = logging.getLogger(__name__)


class EmbeddingService:
    """
    Handles embedding generation using bge-m3
    """
    def __init__(
        self,
        ollama: OllamaManager,
        semaphore: asyncio.Semaphore
    ):
        """
        Initialize embedding service

        Args:
            ollama: OllamaManager instance
            semaphore: Semaphore for concurrency control
        """
        self._ollama = ollama
        self._semaphore = semaphore

    @retry(
        retry = retry_if_exception_type(
            (httpx.TimeoutException,
             httpx.ConnectError)
        ),
        stop = stop_after_attempt(3),
        wait = wait_exponential(multiplier = 1,
                                min = 2,
                                max = 30),
    )
    async def generate_embedding(self, text: str) -> list[float]:
        """
        Generate embedding vector from text using bge-m3 via Ollama

        Args:
            text: Text to embed (vision description or search query)

        Returns:
            1024-dimensional embedding vector
        """
        try:
            if len(text) > config.MAX_EMBEDDING_TEXT_LENGTH:
                text = text[: config.MAX_EMBEDDING_TEXT_LENGTH] + "..."
                logger.warning(
                    f"Truncated text from {len(text)} to "
                    f"{config.MAX_EMBEDDING_TEXT_LENGTH} chars"
                )

            async with self._semaphore:
                client = await self._ollama.get_client()
                response = await client.embeddings(
                    model = config.settings.local_embedding_model,
                    prompt = text,
                )

                embedding = response["embedding"]

                logger.debug(
                    f"Embedding type: {type(embedding).__name__}, "
                    f"length: {len(embedding)}"
                )

                if len(embedding
                       ) != config.settings.local_embedding_dimensions:
                    raise EmbeddingError(
                        f"Invalid embedding dimensions: {len(embedding)} "
                        f"(expected {config.settings.local_embedding_dimensions})"
                    )

                return embedding  # type: ignore[no-any-return]

        except ResponseError as e:
            if e.status_code == status.HTTP_404_NOT_FOUND:
                raise EmbeddingError(
                    f"Model not loaded. Run: ollama pull "
                    f"{config.settings.local_embedding_model}"
                ) from e
            raise EmbeddingError(
                f"Embedding model error: {e.e
```

---

## File Overview

### embedding.py

**Purpose:**
This Python module is responsible for generating embeddings using the `bge-m3` model via the Ollama API. It ensures robustness through retries and concurrency control, making it a critical component of the AI services in the project.

**Key Exports & Public Interface:**
- **EmbeddingService:** A class that handles embedding generation.
  - `__init__`: Initializes the service with an `OllamaManager` instance and a semaphore for concurrency control.
  - `generate_embedding`: Asynchronously generates embeddings from text, handling retries and exceptions.
  - `create_query_embedding`: A convenience method to generate embeddings for search queries.
  - `batch_generate`: Generates embeddings for multiple texts concurrently.

**Project Integration:**
This service is part of the AI module in the backend, providing essential functionality for generating embeddings used across various parts of the application. It integrates with the Ollama API and leverages a semaphore to manage concurrency effectively.

**Design Decisions:**
- **Retries:** The `generate_embedding` method uses retries to handle transient errors like timeouts or connection issues.
- **Concurrency Control:** A semaphore is used to limit the number of concurrent requests, ensuring efficient use of resources.
- **Error Handling:** Robust error handling ensures that failed embeddings are logged and reattempted where possible, with specific error messages for different failure cases.

This file plays a crucial role in providing reliable embedding generation capabilities, which are essential for various AI-driven functionalities within the project.

---

*Generated by CodeWorm on 2026-02-20 13:36*
