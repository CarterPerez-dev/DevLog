# service

**Type:** Code Evolution
**Repository:** angelamos-3d
**File:** backend/app/chat/service.py
**Language:** python
**Lines:** 1-1
**Complexity:** 0.0

---

## Source Code

```python
Commit: a61e4cd8
Message: feat: complete mvp v1.0.0
Author: CarterPerez-dev
File: backend/app/chat/service.py
Change type: new file

Diff:
@@ -0,0 +1,77 @@
+"""
+Â©AngelaMos | 2026
+service.py
+"""
+
+from collections.abc import AsyncGenerator
+from typing import Any
+
+import httpx
+
+from app.chat.schemas import ChatResponse, Message
+from app.config import ANGELA_SYSTEM_PROMPT, settings
+from app.core.exceptions import OllamaConnectionError
+
+
+SYSTEM_MESSAGE = Message(role = "system", content = ANGELA_SYSTEM_PROMPT)
+
+
+def build_payload(messages: list[Message], stream: bool) -> dict[str, Any]:
+    """
+    Build Ollama request payload with system prompt and options.
+    """
+    all_messages = [SYSTEM_MESSAGE, *messages]
+    return {
+        "model": settings.OLLAMA_MODEL,
+        "messages": [m.model_dump() for m in all_messages],
+        "stream": stream,
+        "options": {
+            "temperature": settings.OLLAMA_TEMPERATURE,
+            "num_predict": settings.OLLAMA_MAX_TOKENS,
+        },
+    }
+
+
+async def stream_chat(messages: list[Message]) -> AsyncGenerator[bytes, None]:
+    """
+    Stream chat responses from Ollama.
+    """
+    payload = build_payload(messages, stream = True)
+
+    try:
+        async with httpx.AsyncClient() as client:  # noqa: SIM117
+            async with client.stream(
+                    "POST",
+                    f"{settings.OLLAMA_HOST}/api/chat",
+                    json = payload,
+                    timeout = settings.OLLAMA_TIMEOUT,
+            ) as response:
+                async for line in response.aiter_lines():
+                    if line:
+                        yield line.encode() + b"\n"
+    except httpx.ConnectError as e:
+        raise OllamaConnectionError() from e
+
+
+async def chat(messages: list[Message]) -> ChatResponse:
+    """
+    Send chat request to Ollama and return complete response.
+    """
+    payload = build_payload(messages, stream = False)
+
+    try:
+        async with httpx.AsyncClient() as client:
+            response = await client.post(
+                f"{settings.OLLAMA_HOST}/api/chat",
+                json = payload,
+                timeout = settings.OLLAMA_TIMEOUT,
+            )
+            data = response.json()
+
+        return ChatResponse(
+            message = Message(**data["message"]),
+            model = data["model"],
+            done = data["done"],
+        )
+    except httpx.ConnectError as e:
+        raise OllamaConnectionError() from e

```

---

## Code Evolution

### Change Analysis

**What was Changed:**
The commit `a61e4cd8` introduces a new file `service.py` in the `backend/app/chat/` directory, implementing two main functions: `stream_chat` and `chat`. The file also defines a `build_payload` function to construct request payloads for Ollama API calls.

**Why it was Likely Changed:**
This change likely aims to complete MVP v1.0.0 by adding essential functionality for interacting with the Ollama chat API, specifically handling both streaming and non-streaming requests.

**Impact on Behavior:**
- `stream_chat` allows real-time interaction with the chat model.
- `chat` returns a full response from the model in one go.
Both functions use configurable settings like temperature and maximum tokens, ensuring flexibility in request parameters.

**Risks or Concerns:**
- The use of `httpx.AsyncClient()` without explicit context manager handling could lead to potential issues if not managed properly. However, the `with` statement is correctly used here.
- Raising a custom exception `OllamaConnectionError` on connection errors ensures clear error handling and reporting.

Overall, this change significantly enhances the chat service's capabilities by providing both streaming and non-streaming interaction modes with Ollama.

---

*Generated by CodeWorm on 2026-02-23 18:41*
