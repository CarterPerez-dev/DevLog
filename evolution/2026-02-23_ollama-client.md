# ollama.client

**Type:** Code Evolution
**Repository:** angelamos-3d
**File:** frontend/src/api/ollama.client.ts
**Language:** typescript
**Lines:** 1-1
**Complexity:** 0.0

---

## Source Code

```typescript
Commit: a61e4cd8
Message: feat: complete mvp v1.0.0
Author: CarterPerez-dev
File: frontend/src/api/ollama.client.ts
Change type: new file

Diff:
@@ -0,0 +1,74 @@
+// ===================
+// Â© AngelaMos | 2026
+// ollama.client.ts
+// ===================
+
+import { getAngelaConfig } from "../config";
+import type { OllamaMessage, OllamaStreamChunk } from "../types";
+
+export async function* streamChat(
+	messages: OllamaMessage[],
+	signal?: AbortSignal,
+): AsyncGenerator<string, string, unknown> {
+	const config = getAngelaConfig();
+
+	const response = await fetch(`${config.api.baseUrl}/v1/chat`, {
+		method: "POST",
+		headers: { "Content-Type": "application/json" },
+		body: JSON.stringify({
+			messages,
+			stream: true,
+		}),
+		signal,
+	});
+
+	if (!response.ok) {
+		throw new Error(`Chat request failed: ${response.status}`);
+	}
+
+	const reader = response.body?.getReader();
+	if (!reader) {
+		throw new Error("No response body");
+	}
+
+	const decoder = new TextDecoder();
+	let fullResponse = "";
+
+	while (true) {
+		const { done, value } = await reader.read();
+		if (done) break;
+
+		const chunk = decoder.decode(value, { stream: true });
+		const lines = chunk.split("\n").filter((line) => line.trim());
+
+		for (const line of lines) {
+			try {
+				const data: OllamaStreamChunk = JSON.parse(line);
+				if (data.message?.content) {
+					fullResponse += data.message.content;
+					yield data.message.content;
+				}
+			} catch {}
+		}
+	}
+
+	return fullResponse;
+}
+
+export async function chat(messages: OllamaMessage[]): Promise<string> {
+	let result = "";
+	for await (const chunk of streamChat(messages)) {
+		result += chunk;
+	}
+	return result;
+}
+
+export async function checkHealth(): Promise<boolean> {
+	const config = getAngelaConfig();
+	try {
+		const response = await fetch(`${config.api.baseUrl}/health`);
+		return response.ok;
+	} catch {
+		return false;
+	}
+}

```

---

## Code Evolution

### Change Analysis for Commit `a61e4cd8`

**What was Changed:**
A new file, `ollama.client.ts`, was added to the repository. This file introduces three main functions:
- `streamChat`: An asynchronous generator function that streams chat responses from an API.
- `chat`: A synchronous wrapper around `streamChat` that collects all chunks into a single string.
- `checkHealth`: A function to check the health of the API endpoint.

**Why it was Likely Changed:**
The addition of these functions likely aims to support real-time communication with the Ollama API, allowing for dynamic and continuous updates during chat interactions. The `streamChat` function specifically handles streaming responses, making it suitable for applications requiring immediate feedback from the server.

**Impact on Behavior:**
- `streamChat` enables real-time processing of chat messages by yielding each chunk as it arrives.
- `chat` simplifies usage by collecting all chunks into a single string.
- `checkHealth` provides a simple way to verify API availability, ensuring that the application can gracefully handle service outages.

**Risks or Concerns:**
- The use of an asynchronous generator in `streamChat` might introduce complexity for developers unfamiliar with this pattern.
- Error handling could be improved; currently, errors are caught but not always properly logged or handled.
- The reliance on JSON parsing within the loop may lead to potential security issues if improperly formatted data is received.

---

*Generated by CodeWorm on 2026-02-23 19:19*
