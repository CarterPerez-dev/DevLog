# ollama.client

**Type:** Code Evolution
**Repository:** angelamos-3d
**File:** frontend/src/api/ollama.client.ts
**Language:** typescript
**Lines:** 1-1
**Complexity:** 0.0

---

## Source Code

```typescript
Commit: a61e4cd8
Message: feat: complete mvp v1.0.0
Author: CarterPerez-dev
File: frontend/src/api/ollama.client.ts
Change type: new file

Diff:
@@ -0,0 +1,74 @@
+// ===================
+// Â© AngelaMos | 2026
+// ollama.client.ts
+// ===================
+
+import { getAngelaConfig } from "../config";
+import type { OllamaMessage, OllamaStreamChunk } from "../types";
+
+export async function* streamChat(
+	messages: OllamaMessage[],
+	signal?: AbortSignal,
+): AsyncGenerator<string, string, unknown> {
+	const config = getAngelaConfig();
+
+	const response = await fetch(`${config.api.baseUrl}/v1/chat`, {
+		method: "POST",
+		headers: { "Content-Type": "application/json" },
+		body: JSON.stringify({
+			messages,
+			stream: true,
+		}),
+		signal,
+	});
+
+	if (!response.ok) {
+		throw new Error(`Chat request failed: ${response.status}`);
+	}
+
+	const reader = response.body?.getReader();
+	if (!reader) {
+		throw new Error("No response body");
+	}
+
+	const decoder = new TextDecoder();
+	let fullResponse = "";
+
+	while (true) {
+		const { done, value } = await reader.read();
+		if (done) break;
+
+		const chunk = decoder.decode(value, { stream: true });
+		const lines = chunk.split("\n").filter((line) => line.trim());
+
+		for (const line of lines) {
+			try {
+				const data: OllamaStreamChunk = JSON.parse(line);
+				if (data.message?.content) {
+					fullResponse += data.message.content;
+					yield data.message.content;
+				}
+			} catch {}
+		}
+	}
+
+	return fullResponse;
+}
+
+export async function chat(messages: OllamaMessage[]): Promise<string> {
+	let result = "";
+	for await (const chunk of streamChat(messages)) {
+		result += chunk;
+	}
+	return result;
+}
+
+export async function checkHealth(): Promise<boolean> {
+	const config = getAngelaConfig();
+	try {
+		const response = await fetch(`${config.api.baseUrl}/health`);
+		return response.ok;
+	} catch {
+		return false;
+	}
+}

```

---

## Code Evolution

### Change Analysis for Commit `a61e4cd8`

**What was Changed:**
A new file, `ollama.client.ts`, was added to the repository. This file contains three main functions:
- `streamChat`: An asynchronous generator function that streams chat responses from an API.
- `chat`: A wrapper function that uses `streamChat` to concatenate all chunks of text into a single string.
- `checkHealth`: A function to check the health status of the API.

**Why it was Likely Changed:**
This change likely aims to implement the MVP (Minimum Viable Product) v1.0.0 for interacting with an Ollama API, focusing on chat functionality and basic health checks. The introduction of these functions suggests a need for real-time communication capabilities and monitoring of the backend service.

**Impact on Behavior:**
- `streamChat` allows for efficient handling of large responses by processing them in chunks.
- `chat` simplifies usage by providing a straightforward interface to stream chat messages.
- `checkHealth` ensures that the API is operational before proceeding with any requests, enhancing reliability.

**Risks or Concerns:**
- The use of `try-catch` blocks without proper error handling could mask issues. Consider implementing more robust error management.
- The `streamChat` function currently does not handle all possible errors gracefully, which might lead to unexpected behavior if the API returns non-standard responses.
- The absence of type annotations for some variables (e.g., `line`) may cause runtime errors in a production environment.

Overall, this change significantly enhances the project's capability to interact with an external API and ensures basic service availability checks.

---

*Generated by CodeWorm on 2026-02-23 19:19*
