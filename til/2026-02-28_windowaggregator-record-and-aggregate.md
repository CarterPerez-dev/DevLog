# WindowAggregator.record_and_aggregate

**Type:** Today I Learned
**Repository:** Cybersecurity-Projects
**File:** PROJECTS/advanced/ai-threat-detection/backend/app/core/features/aggregator.py
**Language:** python
**Lines:** 33-121
**Complexity:** 3.0

---

## Source Code

```python
async def record_and_aggregate(
        self,
        ip: str,
        request_id: str,
        path: str,
        path_depth: int,
        method: str,
        status_code: int,
        user_agent: str,
        response_size: int,
        timestamp: float,
    ) -> dict[str, float]:
        """
        Record a request into Redis sorted sets and return all 12
        per-IP windowed features.
        """
        prefix = f"ip:{ip}"
        keys = {
            "requests": f"{prefix}:requests",
            "paths": f"{prefix}:paths",
            "statuses": f"{prefix}:statuses",
            "uas": f"{prefix}:uas",
            "sizes": f"{prefix}:sizes",
            "methods": f"{prefix}:methods",
            "depths": f"{prefix}:depths",
        }

        trim_boundary = timestamp - KEY_TTL
        w1m = timestamp - WINDOW_1M
        w5m = timestamp - WINDOW_5M
        w10m = timestamp - WINDOW_10M

        pipe = self._redis.pipeline()

        pipe.zadd(keys["requests"], {request_id: timestamp})
        pipe.zadd(keys["paths"], {_hash_member(path): timestamp})
        pipe.zadd(keys["statuses"], {f"{status_code}:{request_id}": timestamp})
        pipe.zadd(keys["uas"], {_hash_member(user_agent): timestamp})
        pipe.zadd(keys["sizes"], {f"{response_size}:{request_id}": timestamp})
        pipe.zadd(keys["methods"], {f"{method}:{request_id}": timestamp})
        pipe.zadd(keys["depths"], {f"{path_depth}:{request_id}": timestamp})

        for key in keys.values():
            pipe.zremrangebyscore(key, "-inf", trim_boundary)

        pipe.zcount(keys["requests"], w1m, "+inf")
        pipe.zcount(keys["requests"], w5m, "+inf")
        pipe.zcount(keys["requests"], w10m, "+inf")
        pipe.zcount(keys["paths"], w5m, "+inf")
        pipe.zcount(keys["uas"], w10m, "+inf")
        pipe.zrangebyscore(keys["statuses"], w5m, "+inf")
        pipe.zrangebyscore(keys["sizes"], w5m, "+inf")
        pipe.zrangebyscore(keys["methods"], w5m, "+inf")
        pipe.zrangebyscore(keys["depths"], w5m, "+inf")
        pipe.zrangebyscore(keys["requests"], w10m, "+inf", withscores=True)

        for key in keys.values():
            pipe.expire(key, KEY_TTL)

        results = await pipe.execute()

        read_start = len(keys) * 2
        req_count_1m = results[read_start]
        req_count_5m = results[read_start + 1]
        req_count_10m = results[read_start + 2]
        unique_paths_5m = results[read_start + 3]
        unique_uas_10m = results[read_start + 4]
        statuses_5m = results[read_start + 5]
        sizes_5m = results[read_start + 6]
        methods_5m = results[read_start + 7]
        depths_5m = results[read_start + 8]
        requests_with_scores = results[read_start + 9]

        irt_mean, irt_std = _inter_request_time_stats(requests_with_scores)

        return {
            "req_count_1m": float(req_count_1m),
            "req_count_5m": float(req_count_5m),
            "req_count_10m": float(req_count_10m),
            "error_rate_5m":
```

---

## Today I Learned

TIL: The `record_and_aggregate` function uses Redis sorted sets to efficiently track and aggregate request metrics. By leveraging pipelining, it minimizes the number of round trips to the database, making the function both performant and scalable.

```python
pipe = self._redis.pipeline()
# ... multiple zadd and zremrangebyscore commands ...
results = await pipe.execute()
```

Pipelining allows for batching commands, reducing latency and improving overall performance.

---

*Generated by CodeWorm on 2026-02-28 07:16*
