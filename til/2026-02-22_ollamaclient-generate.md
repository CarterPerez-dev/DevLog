# OllamaClient.generate

**Type:** Today I Learned
**Repository:** CodeWorm
**File:** codeworm/llm/client.py
**Language:** python
**Lines:** 147-212
**Complexity:** 11.0

---

## Source Code

```python
async def generate(
        self,
        prompt: str,
        system: str | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> GenerationResult:
        """
        Generate text from a prompt
        """
        client = await self._get_client()

        options = {
            "temperature": temperature or self.settings.temperature,
            "num_predict": max_tokens or self.settings.num_predict,
            "num_ctx": self.settings.num_ctx,
        }

        payload = {
            "model": self.settings.model,
            "prompt": prompt,
            "stream": False,
            "keep_alive": self.settings.keep_alive,
            "options": options,
        }

        if system:
            payload["system"] = system

        try:
            response = await client.post("/api/generate", json = payload)

            if response.status_code != 200:
                error_text = response.text
                if "out of memory" in error_text.lower(
                ) or "cuda" in error_text.lower():
                    raise OllamaModelError(f"Model OOM: {error_text}")
                raise OllamaError(f"Generation failed: {error_text}")

            data = response.json()

            prompt_tokens = data.get("prompt_eval_count", 0)
            completion_tokens = data.get("eval_count", 0)
            total_duration = data.get("total_duration", 0) / 1_000_000

            tokens_per_sec = 0.0
            if total_duration > 0 and completion_tokens > 0:
                tokens_per_sec = completion_tokens / (total_duration / 1000)

            return GenerationResult(
                text = data.get("response",
                                ""),
                model = data.get("model",
                                 self.settings.model),
                prompt_tokens = prompt_tokens,
                completion_tokens = completion_tokens,
                total_duration_ms = int(total_duration),
                tokens_per_second = tokens_per_sec,
            )

        except httpx.ConnectError as e:
            raise OllamaConnectionError(
                f"Cannot connect to Ollama at {self.base_url}: {e}"
            ) from e
        except httpx.TimeoutException as e:
            raise OllamaTimeoutError(f"Request timed out: {e}") from e
```

---

## Today I Learned

TIL: In `OllamaClient.generate`, default parameter values are set using dictionary unpacking, making the function signature clean and flexible. This allows `temperature` and `max_tokens` to use class settings if not provided, enhancing reusability.

```python
options = {
    "temperature": temperature or self.settings.temperature,
    "num_predict": max_tokens or self.settings.num_predict,
    "num_ctx": self.settings.num_ctx,
}
```

This pattern keeps the function call simple while ensuring default values are always available.

---

*Generated by CodeWorm on 2026-02-22 11:53*
