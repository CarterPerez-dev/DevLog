# AnalyzeSchema

**Type:** Today I Learned
**Repository:** angelamos-operations
**File:** CertGamesDB-Argos/go-backend/internal/mongodb/collections.go
**Language:** go
**Lines:** 208-281
**Complexity:** 10.0

---

## Source Code

```go
func (r *CollectionsRepository) AnalyzeSchema(ctx context.Context, dbName, collName string, sampleSize int) (*SchemaAnalysis, error) {
	db := r.client.client.Database(dbName)
	coll := db.Collection(collName)

	totalDocs, err := coll.EstimatedDocumentCount(ctx)
	if err != nil {
		return nil, fmt.Errorf("count documents: %w", err)
	}

	if sampleSize <= 0 {
		sampleSize = 1000
	}
	if int64(sampleSize) > totalDocs {
		sampleSize = int(totalDocs)
	}

	pipeline := mongo.Pipeline{
		{{Key: "$sample", Value: bson.D{{Key: "size", Value: sampleSize}}}},
	}

	cursor, err := coll.Aggregate(ctx, pipeline)
	if err != nil {
		return nil, fmt.Errorf("sample documents: %w", err)
	}
	defer cursor.Close(ctx)

	fieldMap := make(map[string]*fieldInfo)
	var sampledCount int64

	for cursor.Next(ctx) {
		var doc bson.M
		if err := cursor.Decode(&doc); err != nil {
			continue
		}
		sampledCount++
		analyzeDocument("", doc, fieldMap)
	}

	var fields []FieldSchema
	for name, info := range fieldMap {
		typeList := make([]string, 0, len(info.types))
		for t := range info.types {
			typeList = append(typeList, t)
		}
		sort.Strings(typeList)

		coverage := float64(info.count) / float64(sampledCount) * 100

		samples := info.samples
		if len(samples) > 5 {
			samples = samples[:5]
		}

		fields = append(fields, FieldSchema{
			Name:         name,
			Types:        typeList,
			Coverage:     coverage,
			Count:        info.count,
			TotalDocs:    sampledCount,
			SampleValues: samples,
		})
	}

	sort.Slice(fields, func(i, j int) bool {
		return fields[i].Coverage > fields[j].Coverage
	})

	return &SchemaAnalysis{
		CollectionName: collName,
		TotalDocuments: totalDocs,
		SampleSize:     sampledCount,
		Fields:         fields,
	}, nil
}
```

---

## Today I Learned

TIL: In `CollectionsRepository.AnalyzeSchema`, the function intelligently handles sampling and schema analysis. It uses MongoDB's `$sample` aggregation stage to fetch a representative subset of documents, ensuring efficient processing even with large collections. This approach combines error handling, context-aware operations, and thoughtful data structuring to provide a comprehensive schema analysis.

```go
cursor, err := coll.Aggregate(ctx, pipeline)
if err != nil {
	return nil, fmt.Errorf("sample documents: %w", err)
}
```

This snippet showcases robust error management and context propagation.

---

*Generated by CodeWorm on 2026-02-20 21:01*
